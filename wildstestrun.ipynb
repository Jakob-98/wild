{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import parameters\n",
    "params = parameters.paramStore.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import math\n",
    "import sys\n",
    "from wilds import get_dataset\n",
    "from wilds.datasets.wilds_dataset import WILDSSubset\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "import torchvision.transforms as transforms\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_pretrained_model = False\n",
    "img_size          = params.img_size\n",
    "location          = None\n",
    "batch_size        = params.batch_size\n",
    "epochs            = params.epocs\n",
    "eval_per_epochs   = 1\n",
    "device            = params.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'val': 1, 'test': 2, 'id_val': 3, 'id_test': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwildcam = get_dataset(dataset=\"iwildcam\", download=False)\n",
    "grouper = CombinatorialGrouper(iwildcam, ['location'])\n",
    "iwildcam.split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset= iwildcam.get_subset(\n",
    "  \"train\",\n",
    "  transform=transforms.Compose(\n",
    "      [transforms.Resize((img_size, img_size)), transforms.ToTensor()]\n",
    "  ),\n",
    ")\n",
    "\n",
    "testset= iwildcam.get_subset(\n",
    "  \"test\",\n",
    "  transform=transforms.Compose(\n",
    "      [transforms.Resize((img_size, img_size)), transforms.ToTensor()]\n",
    "  ),\n",
    ")\n",
    "\n",
    "valset = iwildcam.get_subset(\n",
    "  \"val\",\n",
    "  transform=transforms.Compose(\n",
    "      [transforms.Resize((img_size, img_size)), transforms.ToTensor()]\n",
    "  ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels, _ = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(device, model, data_loader, dataset, text=None):\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  true_labels = []\n",
    "  metadatas = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for data in data_loader:\n",
    "          inputs, labels, metadata = data\n",
    "          inputs = inputs.to(device).float()\n",
    "          true_labels += list(labels.to(device).cpu().numpy())\n",
    "          metadatas += list(metadata.to(device).cpu().numpy())\n",
    "          outputs = model(inputs)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          predictions += list(predicted.to(device).cpu().numpy())\n",
    "\n",
    "  eval = dataset.eval(torch.tensor(predictions), torch.tensor(true_labels), torch.tensor(metadatas))\n",
    "  if text != None:\n",
    "    print(text + eval[1])\n",
    "  return eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset):\n",
    "  targets = {}\n",
    "  for i in dataset.indices:\n",
    "    targets[i] = int(dataset.dataset[i][1])\n",
    "  return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = get_targets(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/archive/torchhub.zip\" to C:\\Users\\jakob/.cache\\torch\\hub\\torchhub.zip\n",
      "C:\\Users\\jakob/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "C:\\Users\\jakob/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/resnet50_pyt_amp/versions/20.06.0/files/nvidia_resnet50_200821.pth.tar\" to C:\\Users\\jakob/.cache\\torch\\hub\\checkpoints\\nvidia_resnet50_200821.pth.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cadf955e15c4010b7dd81d2428374e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = testset\n",
    "location_indices = []\n",
    "for i in dataset.indices:\n",
    "  location_i = int(grouper.metadata_to_group(torch.stack([dataset.dataset[i][2]]))[0])\n",
    "  if location_i == location or location is None:\n",
    "    location_indices.append(i)\n",
    "dataset.indices = location_indices\n",
    "unfiltered_targets = get_targets(dataset)\n",
    "class_freq = Counter(unfiltered_targets.values())\n",
    "filter_classes = []\n",
    "for class_, freq in class_freq.items():\n",
    "  if freq == 1:\n",
    "    filter_classes.append(class_)\n",
    "indices = []\n",
    "for i in dataset.indices:\n",
    "  if unfiltered_targets[i] not in filter_classes:\n",
    "    indices.append(i)\n",
    "dataset.indices = indices\n",
    "targets = get_targets(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, valid_indices = train_test_split(testset.indices, test_size=0.5, stratify=list(targets.values()))\n",
    "train_data = WILDSSubset(dataset.dataset, train_indices, dataset.transform)\n",
    "valid_data = WILDSSubset(dataset.dataset, valid_indices, dataset.transform)\n",
    "train_loader = get_train_loader('standard', train_data, batch_size)\n",
    "valid_loader = get_eval_loader('standard', valid_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, dataset.n_classes)\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_conv = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while epoch < epochs + 1:\n",
    "    # Train\n",
    "    train(device, model, train_loader, criterion, optimizer_conv)\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % eval_per_epochs == 0 or epoch == 1:\n",
    "      evaluate(device, model, train_loader, dataset, f\"Epoch {epoch} train set\\n\")\n",
    "      f1 = evaluate(device, model, valid_loader, dataset, f\"Epoch {epoch} valid set\\n\")['F1-macro_all']\n",
    "      print(\"--------------\")\n",
    "\n",
    "      if f1 > best_f1[0]: # store best model so far, for later, based on best val auc\n",
    "          best_model = copy.deepcopy(model)\n",
    "          best_f1 = [f1, epoch]\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  188, 14022,  2013,  ...,     1,     0,     0],\n",
       "        [  287, 13916,  2013,  ...,    26,    79,     0],\n",
       "        [  120,  8505,  2013,  ...,    25,    15,     0],\n",
       "        ...,\n",
       "        [  163, 21053,  2015,  ...,    42,    27,     0],\n",
       "        [  187, 34468,  2013,  ...,    53,     0,     0],\n",
       "        [  288, 12740,  2013,  ...,    12,    36,     0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.metadata_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=560x448>,\n",
       " tensor(146),\n",
       " tensor([   2, 2157, 2013,    6,   10,    6,   53,   18,  146,    0]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203029"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d488aad3bd47f31cc49211d239eea484ef57a3647b66c4c13e8e3612e9e7defd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('wildsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
