{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/microsoft/CameraTraps/blob/f3f192a4145725f9cf596d0e9f49c4008639c6e9/data_management/lila/download_lila_subset.py#L56\n",
    "# download_lila_subset.py\n",
    "#\n",
    "# Example of how to download a list of files from LILA, e.g. all the files\n",
    "# in a data set corresponding to a particular species.\n",
    "#\n",
    "\n",
    "#%% Constants and imports\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bypassing download of already-downloaded file lila_sas_urls.txt\n",
      "Bypassing download of already-downloaded file swg_camera_traps.zip\n",
      "c:\\temp\\lila\\metadata\\swg_camera_traps.json already unzipped\n",
      "Found 1 matching categories for data set SWG Camera Traps:\n",
      "leopard_cat\n",
      "Selected 266 of 2039657 images for dataset SWG Camera Traps\n",
      "Taking the first 10 of 266 images for SWG Camera Traps\n",
      "Found 10 images to download\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LILA camera trap master metadata file\n",
    "metadata_url = 'http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt'\n",
    "\n",
    "# # In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset\n",
    "# datasets_of_interest = ['Missouri Camera Traps','ENA24','Caltech Camera Traps']\n",
    "\n",
    "# # All lower-case; we'll convert category names to lower-case when comparing\n",
    "# species_of_interest = ['red_fox','fox','grey fox','red fox']\n",
    "\n",
    "# # We'll write images, metadata downloads, and temporary files here\n",
    "# lila_local_base = r'c:\\temp\\lila'\n",
    "\n",
    "# In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset\n",
    "datasets_of_interest = ['SWG Camera Traps']\n",
    "\n",
    "# All lower-case; we'll convert category names to lower-case when comparing\n",
    "species_of_interest = ['leopard_cat']\n",
    "\n",
    "# We'll write images, metadata downloads, and temporary files here\n",
    "lila_local_base = r'c:\\temp\\lila'\n",
    "\n",
    "\n",
    "output_dir = os.path.join(lila_local_base,'lila_downloads_by_species')\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "metadata_dir = os.path.join(lila_local_base,'metadata')\n",
    "os.makedirs(metadata_dir,exist_ok=True)\n",
    "\n",
    "# We will demonstrate two approaches to downloading, one that loops over files\n",
    "# and downloads directly in Python, another that uses AzCopy.\n",
    "#\n",
    "# AzCopy will generally be more performant and supports resuming if the \n",
    "# transfers are interrupted.  This script assumes that azcopy is on the system path.\n",
    "use_azcopy_for_download = True\n",
    "\n",
    "overwrite_files = False\n",
    "\n",
    "# Number of concurrent download threads (when not using AzCopy) (AzCopy does its\n",
    "# own magical parallelism)\n",
    "n_download_threads = 50\n",
    "\n",
    "max_images_per_dataset = 10\n",
    "\n",
    "\n",
    "#%% Support functions\n",
    "\n",
    "def download_url(url, destination_filename=None, force_download=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Download a URL (defaulting to a temporary file)\n",
    "    \"\"\"\n",
    "    \n",
    "    if destination_filename is None:\n",
    "        temp_dir = os.path.join(tempfile.gettempdir(),'lila')\n",
    "        os.makedirs(temp_dir,exist_ok=True)\n",
    "        url_as_filename = url.replace('://', '_').replace('.', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "            \n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        print('Bypassing download of already-downloaded file {}'.format(os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    \n",
    "    if verbose:\n",
    "        print('Downloading file {} to {}'.format(os.path.basename(url),destination_filename),end='')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(destination_filename),exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, destination_filename)  \n",
    "    assert(os.path.isfile(destination_filename))\n",
    "    \n",
    "    if verbose:\n",
    "        nBytes = os.path.getsize(destination_filename)    \n",
    "        print('...done, {} bytes.'.format(nBytes))\n",
    "        \n",
    "    return destination_filename\n",
    "\n",
    "\n",
    "def download_relative_filename(url, output_base, verbose=False):\n",
    "    \"\"\"\n",
    "    Download a URL to output_base, preserving relative path\n",
    "    \"\"\"\n",
    "    \n",
    "    p = urlparse(url)\n",
    "    # remove the leading '/'\n",
    "    assert p.path.startswith('/'); relative_filename = p.path[1:]\n",
    "    destination_filename = os.path.join(output_base,relative_filename)\n",
    "    download_url(url, destination_filename, verbose=verbose)\n",
    "    \n",
    "\n",
    "def unzip_file(input_file, output_folder=None):\n",
    "    \"\"\"\n",
    "    Unzip a zipfile to the specified output folder, defaulting to the same location as\n",
    "    the input file    \n",
    "    \"\"\"\n",
    "    \n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.dirname(input_file)\n",
    "        \n",
    "    with zipfile.ZipFile(input_file, 'r') as zf:\n",
    "        zf.extractall(output_folder)\n",
    "\n",
    "\n",
    "#%% Download and parse the metadata file\n",
    "\n",
    "# Put the master metadata file in the same folder where we're putting images\n",
    "p = urlparse(metadata_url)\n",
    "metadata_filename = os.path.join(metadata_dir,os.path.basename(p.path))\n",
    "download_url(metadata_url, metadata_filename)\n",
    "\n",
    "# Read lines from the master metadata file\n",
    "with open(metadata_filename,'r') as f:\n",
    "    metadata_lines = f.readlines()\n",
    "metadata_lines = [s.strip() for s in metadata_lines]\n",
    "\n",
    "# Parse those lines into a table\n",
    "metadata_table = {}\n",
    "\n",
    "for s in metadata_lines:\n",
    "    \n",
    "    if len(s) == 0 or s[0] == '#':\n",
    "        continue\n",
    "    \n",
    "    # Each line in this file is name/base_url/json_url/[box_url]\n",
    "    tokens = s.split(',')\n",
    "    assert len(tokens)==4\n",
    "    url_mapping = {'sas_url':tokens[1],'json_url':tokens[2]}\n",
    "    metadata_table[tokens[0]] = url_mapping\n",
    "    \n",
    "    assert 'https' not in tokens[0]\n",
    "    assert 'https' in url_mapping['sas_url']\n",
    "    assert 'https' in url_mapping['json_url']\n",
    "\n",
    "\n",
    "#%% Download and extract metadata for the datasets we're interested in\n",
    "\n",
    "for ds_name in datasets_of_interest:\n",
    "    \n",
    "    assert ds_name in metadata_table\n",
    "    json_url = metadata_table[ds_name]['json_url']\n",
    "    \n",
    "    p = urlparse(json_url)\n",
    "    json_filename = os.path.join(metadata_dir,os.path.basename(p.path))\n",
    "    download_url(json_url, json_filename)\n",
    "    \n",
    "    # Unzip if necessary\n",
    "    if json_filename.endswith('.zip'):\n",
    "        \n",
    "        with zipfile.ZipFile(json_filename,'r') as z:\n",
    "            files = z.namelist()\n",
    "        assert len(files) == 1\n",
    "        unzipped_json_filename = os.path.join(metadata_dir,files[0])\n",
    "        if not os.path.isfile(unzipped_json_filename):\n",
    "            unzip_file(json_filename,metadata_dir)        \n",
    "        else:\n",
    "            print('{} already unzipped'.format(unzipped_json_filename))\n",
    "        json_filename = unzipped_json_filename\n",
    "    \n",
    "    metadata_table[ds_name]['json_filename'] = json_filename\n",
    "    \n",
    "# ...for each dataset of interest\n",
    "\n",
    "\n",
    "#%% List of files we're going to download (for all data sets)\n",
    "\n",
    "# Flat list or URLS, for use with direct Python downloads\n",
    "urls_to_download = []\n",
    "\n",
    "# For use with azcopy\n",
    "downloads_by_dataset = {}\n",
    "\n",
    "for ds_name in datasets_of_interest:\n",
    "    \n",
    "    json_filename = metadata_table[ds_name]['json_filename']\n",
    "    sas_url = metadata_table[ds_name]['sas_url']\n",
    "    \n",
    "    # This may or may not be a SAS URL\n",
    "    if '?' in sas_url:\n",
    "        base_url = sas_url.split('?')[0]        \n",
    "        sas_token = sas_url.split('?')[1]\n",
    "        assert not sas_token.startswith('?')\n",
    "    else:\n",
    "        sas_token = ''\n",
    "        base_url = sas_url\n",
    "        \n",
    "    assert not base_url.endswith('/')\n",
    "        \n",
    "    \n",
    "    ## Open the metadata file\n",
    "    \n",
    "    with open(json_filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    categories = data['categories']\n",
    "    for c in categories:\n",
    "        c['name'] = c['name'].lower()\n",
    "    category_id_to_name = {c['id']:c['name'] for c in categories}\n",
    "    annotations = data['annotations']\n",
    "    images = data['images']\n",
    "\n",
    "\n",
    "    ## Build a list of image files (relative path names) that match the target species\n",
    "\n",
    "    category_ids = []\n",
    "    \n",
    "    for species_name in species_of_interest:\n",
    "        matching_categories = list(filter(lambda x: x['name'] == species_name, categories))\n",
    "        if len(matching_categories) == 0:\n",
    "            continue\n",
    "        assert len(matching_categories) == 1\n",
    "        category = matching_categories[0]\n",
    "        category_id = category['id']\n",
    "        category_ids.append(category_id)\n",
    "    \n",
    "    print('Found {} matching categories for data set {}:'.format(len(category_ids),ds_name))\n",
    "    \n",
    "    if len(category_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    for i_category,category_id in enumerate(category_ids):\n",
    "        print(category_id_to_name[category_id],end='')\n",
    "        if i_category != len(category_ids) -1:\n",
    "            print(',',end='')\n",
    "    print('')\n",
    "    \n",
    "    # Retrieve all the images that match that category\n",
    "    image_ids_of_interest = set([ann['image_id'] for ann in annotations if ann['category_id'] in category_ids])\n",
    "    \n",
    "    print('Selected {} of {} images for dataset {}'.format(len(image_ids_of_interest),len(images),ds_name))\n",
    "    \n",
    "    # Retrieve image file names\n",
    "    filenames = [im['file_name'] for im in images if im['id'] in image_ids_of_interest]\n",
    "    assert len(filenames) == len(image_ids_of_interest)\n",
    "    \n",
    "    if max_images_per_dataset is not None and len(filenames) > max_images_per_dataset:\n",
    "        print('Taking the first {} of {} images for {}'.format(\n",
    "            max_images_per_dataset,len(filenames),ds_name))\n",
    "        filenames = filenames[0:max_images_per_dataset]\n",
    "    \n",
    "    # Convert to URLs\n",
    "    for fn in filenames:        \n",
    "        url = base_url + '/' + fn\n",
    "        urls_to_download.append(url)\n",
    "\n",
    "    downloads_by_dataset[ds_name] = {'sas_url':sas_url,'filenames':filenames}\n",
    "    \n",
    "# ...for each dataset\n",
    "\n",
    "print('Found {} images to download'.format(len(urls_to_download)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SWG Camera Traps': {'sas_url': 'https://lilablobssc.blob.core.windows.net/swg-camera-traps',\n",
       "  'filenames': ['public/lao/loc_0040/2018/04/image_00014.jpg',\n",
       "   'public/lao/loc_0041/2018/04/image_00007.jpg',\n",
       "   'public/lao/loc_0041/2018/04/image_00008.jpg',\n",
       "   'public/lao/loc_0053/2017/09/image_00040.jpg',\n",
       "   'public/lao/loc_0056/2017/09/image_00021.jpg',\n",
       "   'public/lao/loc_0058/2018/02/image_00031.jpg',\n",
       "   'public/lao/loc_0059/2018/01/image_00043.jpg',\n",
       "   'public/lao/loc_0299/2020/04/image_00018.jpg',\n",
       "   'public/lao/loc_0349/2020/06/image_00210.jpg',\n",
       "   'public/lao/loc_0349/2020/06/image_00211.jpg']}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloads_by_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for SWG Camera Traps with azcopy\n",
      "azcopy cp \"https://lilablobssc.blob.core.windows.net/swg-camera-traps\" \"c:\\temp\\lila\\lila_downloads_by_species\" --list-of-files \"c:\\temp\\lila\\lila_downloads_by_species\\filenames_swg_camera_traps.txt\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Download those image files\n",
    "\n",
    "if use_azcopy_for_download:\n",
    "    \n",
    "    for ds_name in downloads_by_dataset:\n",
    "    \n",
    "        print('Downloading images for {} with azcopy'.format(ds_name))\n",
    "        sas_url = downloads_by_dataset[ds_name]['sas_url']\n",
    "        filenames = downloads_by_dataset[ds_name]['filenames']\n",
    "    \n",
    "        # We want to use the whole relative path for this script (relative to the base of the container)\n",
    "        # to build the output filename, to make sure that different data sets end up in different folders.\n",
    "        \n",
    "        # This may or may not be a SAS URL\n",
    "        if '?' in sas_url:\n",
    "            base_url = sas_url.split('?')[0]        \n",
    "            sas_token = sas_url.split('?')[1]\n",
    "            assert not sas_token.startswith('?')\n",
    "        else:\n",
    "            sas_token = ''\n",
    "            base_url = sas_url\n",
    "            \n",
    "        assert not base_url.endswith('/')\n",
    "        \n",
    "        p = urlparse(base_url)\n",
    "        account_path = p.scheme + '://' + p.netloc\n",
    "        assert account_path == 'https://lilablobssc.blob.core.windows.net'\n",
    "        \n",
    "        container_and_folder = p.path[1:]\n",
    "        \n",
    "        if len(container_and_folder.split('/')) == 2:\n",
    "            container_name = container_and_folder.split('/')[0]\n",
    "            folder = container_and_folder.split('/',1)[1]\n",
    "            filenames = [folder + '/' + s for s in filenames]\n",
    "        else: \n",
    "            assert(len(container_and_folder.split('/')) == 1)\n",
    "            container_name = container_and_folder\n",
    "        \n",
    "        container_sas_url = account_path + '/' + container_name\n",
    "        if len(sas_token) > 0:\n",
    "            container_sas_url += '?' + sas_token\n",
    "        \n",
    "        # The container name will be included because it's part of the file name\n",
    "        container_output_dir = output_dir # os.path.join(output_dir,container_name)\n",
    "        os.makedirs(container_output_dir,exist_ok=True)\n",
    "        \n",
    "    \n",
    "        # Write out a list of files, and use the azcopy \"list-of-files\" option to download those files\n",
    "        # this azcopy feature is unofficially documented at https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer\n",
    "        az_filename = os.path.join(output_dir, 'filenames_{}.txt'.format(ds_name.lower().replace(' ','_')))\n",
    "        with open(az_filename, 'w') as f:\n",
    "            for fn in filenames:\n",
    "                f.write(fn.replace('\\\\','/') + '\\n')\n",
    "                \n",
    "        cmd = 'azcopy cp \"{0}\" \"{1}\" --list-of-files \"{2}\"'.format(\n",
    "                container_sas_url, container_output_dir, az_filename)            \n",
    "        \n",
    "        print(cmd)\n",
    "        # import clipboard; clipboard.copy(cmd)\n",
    "        \n",
    "        os.system(cmd)\n",
    "    \n",
    "# else:\n",
    "    \n",
    "#     # Loop over files\n",
    "#     print('Downloading images for {0} without azcopy'.format(species_of_interest))\n",
    "    \n",
    "#     if n_download_threads <= 1:\n",
    "    \n",
    "#         for url in tqdm(urls_to_download):        \n",
    "#             download_relative_filename(url,output_dir,verbose=True)\n",
    "        \n",
    "#     else:\n",
    "    \n",
    "#         pool = ThreadPool(n_download_threads)        \n",
    "#         tqdm(pool.imap(lambda s: download_relative_filename(s,output_dir,verbose=False), urls_to_download), total=len(urls_to_download))\n",
    "    \n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        container_sas_url = account_path + '/' + container_name\n",
    "        if len(sas_token) > 0:\n",
    "            container_sas_url += '?' + sas_token\n",
    "        \n",
    "        # The container name will be included because it's part of the file name\n",
    "        container_output_dir = output_dir # os.path.join(output_dir,container_name)\n",
    "        os.makedirs(container_output_dir,exist_ok=True)\n",
    "        \n",
    "    \n",
    "        # Write out a list of files, and use the azcopy \"list-of-files\" option to download those files\n",
    "        # this azcopy feature is unofficially documented at https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer\n",
    "        az_filename = os.path.join(output_dir, 'filenames_{}.txt'.format(ds_name.lower().replace(' ','_')))\n",
    "        with open(az_filename, 'w') as f:\n",
    "            for fn in filenames:\n",
    "                f.write(fn.replace('\\\\','/') + '\\n')\n",
    "                \n",
    "        cmd = 'azcopy cp \"{0}\" \"{1}\" --list-of-files \"{2}\"'.format(\n",
    "                container_sas_url, container_output_dir, az_filename)            \n",
    "        \n",
    "        # import clipboard; clipboard.copy(cmd)\n",
    "        \n",
    "        os.system(cmd)\n",
    "    \n",
    "# else:\n",
    "    \n",
    "#     # Loop over files\n",
    "#     print('Downloading images for {0} without azcopy'.format(species_of_interest))\n",
    "    \n",
    "#     if n_download_threads <= 1:\n",
    "    \n",
    "#         for url in tqdm(urls_to_download):        \n",
    "#             download_relative_filename(url,output_dir,verbose=True)\n",
    "        \n",
    "#     else:\n",
    "    \n",
    "#         pool = ThreadPool(n_download_threads)        \n",
    "#         tqdm(pool.imap(lambda s: download_relative_filename(s,output_dir,verbose=False), urls_to_download), total=len(urls_to_download))\n",
    "    \n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wild",
   "language": "python",
   "name": "wild"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
