{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/microsoft/CameraTraps/blob/main/data_management/lila/create_lila_test_set.py\n",
    "# https://github.com/cindyweng/coco-to-yolo-by-category/blob/5fcd1ae51af89c1c678d903a4aff5d32cba25b0b/coco-to-yolo-by-category.py#L41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter \n",
    "from itertools import groupby\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = './data/islands/images/images/' # refactor to match path naming\n",
    "metadata_path = './data/islands/metadata.json'\n",
    "train_path = './data/ultralytics/images/train/'\n",
    "val_path = './data/ultralytics/images/val/'\n",
    "test_path = './data/ultralytics/images/test/'\n",
    "label_path = './data/ultralytics/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_path) as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5071 images do not exist (images may overlap)\n"
     ]
    }
   ],
   "source": [
    "# Remove images not found\n",
    "ne = []\n",
    "i = 0\n",
    "for img in d['images']:\n",
    "    if not exists(basepath + img['file_name']):\n",
    "        ne.append(img)\n",
    "        i += 1\n",
    "print(\"{} images do not exist (images may overlap)\".format(i))\n",
    "\n",
    "remove_missing_id = set([n.get('id') for n in ne])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(d, remove_missing_id, n_empty= 1000, n_nempty=1000):\n",
    "    n_empty_images_per_dataset = n_empty\n",
    "    n_non_empty_images_per_dataset = n_nempty\n",
    "\n",
    "    category_id_to_name = {c['id']:c['name'] for c in d['categories']}\n",
    "    category_name_to_id = {c['name']:c['id'] for c in d['categories']}\n",
    "\n",
    "\n",
    "    human_category_id = category_name_to_id['human'] if 'human' in category_name_to_id.keys() else -1 # filter out humans\n",
    "\n",
    "\n",
    "    if 'empty' not in category_name_to_id:\n",
    "        print('Warning: no empty images available for {}'.format('dataset'))\n",
    "        empty_category_id = -1\n",
    "        empty_annotations = []\n",
    "        empty_annotations_to_download = []\n",
    "    else:\n",
    "        empty_category_id = category_name_to_id['empty']        \n",
    "        empty_annotations = [ann for ann in d['annotations'] if ann['category_id'] == empty_category_id and ann['image_id'] not in remove_missing_id]\n",
    "        empty_annotations_to_download = random.sample(empty_annotations, n_empty_images_per_dataset)        \n",
    "        \n",
    "    non_empty_annotations = [ann for ann in d['annotations'] if ann['category_id'] not in (empty_category_id, human_category_id) and ann['image_id'] not in remove_missing_id]\n",
    "\n",
    "    non_empty_annotations_to_download = random.sample(non_empty_annotations, n_non_empty_images_per_dataset)\n",
    "    annotations_to_download = empty_annotations_to_download + non_empty_annotations_to_download\n",
    "    image_ids_to_download = set([ann['image_id'] for ann in annotations_to_download])\n",
    "    assert len(image_ids_to_download) == len(set(image_ids_to_download))\n",
    "\n",
    "    images_to_download = []\n",
    "    for im in d['images']:\n",
    "        if im['id'] in image_ids_to_download:\n",
    "            images_to_download.append(im)\n",
    "    assert len(images_to_download) == len(image_ids_to_download)\n",
    "    \n",
    "    return images_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_download = gen_dataset(d, remove_missing_id, 20000, 2000)\n",
    "train, validate, test = np.split(images_to_download, [int(.8*len(images_to_download)), int(.9*len(images_to_download))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trlabelpath, vlabelpath, telabelpath = label_path + \"train/\", label_path + \"val/\", label_path + 'test/'\n",
    "\n",
    "for p in (train_path, val_path, test_path, trlabelpath, vlabelpath, telabelpath):\n",
    "    files = glob.glob(p)\n",
    "    for f in glob.glob(p + '\\*', recursive=True):\n",
    "        if f.endswith('.jpg') or f.endswith('.txt'):\n",
    "            os.remove(f)\n",
    "\n",
    "for im in train: \n",
    "    newfile = shutil.copy2(basepath+im['file_name'], train_path)\n",
    "    newname = os.path.dirname(newfile) + \"/\" + im['file_name'].replace(\"/\", \"-\") # name should match label - multiple 001, 002 files...\n",
    "    os.rename(newfile, newname)\n",
    "for im in validate: \n",
    "    newfile = shutil.copy2(basepath+im['file_name'], val_path)\n",
    "    newname = os.path.dirname(newfile) + \"/\" + im['file_name'].replace(\"/\", \"-\") # name should match label - multiple 001, 002 files...\n",
    "    os.rename(newfile, newname)\n",
    "for im in test: \n",
    "        newfile = shutil.copy2(basepath+im['file_name'], test_path)\n",
    "        newname = os.path.dirname(newfile) + \"/\" + im['file_name'].replace(\"/\", \"-\") # name should match label - multiple 001, 002 files...\n",
    "        os.rename(newfile, newname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncate(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return int(n * multiplier) / multiplier\n",
    "\n",
    "\n",
    "def createLabelsSingle(imageList, basedir, labeldirname, metadata_full):\n",
    "    # For single objects only\n",
    "\n",
    "    ids = [i.get('id') for i in imageList]\n",
    "    # generate lookup for bbox and category id based on image id\n",
    "\n",
    "\n",
    "    print(\"!WARNING: hardcoded fix for islands dataset\")\n",
    "\n",
    "    lookup = {}\n",
    "    for meta in metadata_full[\"annotations\"]:\n",
    "        if meta[\"image_id\"] not in ids: continue\n",
    "\n",
    "        bb = [0, 0, 1919, 1079] #TODO this is hardcoded fix/default for the islands dataset \n",
    "\n",
    "        try:\n",
    "            bb = meta['bbox']\n",
    "        except KeyError:\n",
    "            if meta['category_id'] != 0:\n",
    "                raise KeyError('Keyerror on boundingbox but not an empty image!')\n",
    "\n",
    "        lookup[meta['image_id']] = {\"bbox\": bb, \"category_id\": meta[\"category_id\"]}\n",
    "\n",
    "\n",
    "    for im in imageList:\n",
    "\n",
    "        ann = lookup.get(im['id'])\n",
    "\n",
    "        dw = 1. / im['width']\n",
    "        dh = 1. / im['height']\n",
    "        \n",
    "        \n",
    "        filename = im['file_name'].replace(\".jpg\", \".txt\").replace(\"/\", \"-\")\n",
    "        # print(Path(basedir).parent.__str__() + \"/labels/\" + labeldirname + filename, \"a\")\n",
    "        with open(Path(basedir).parent.parent.__str__() + \"/labels/\" + labeldirname + filename, \"a\") as myfile:\n",
    "            xmin = ann[\"bbox\"][0]\n",
    "            ymin = ann[\"bbox\"][1]\n",
    "            xmax = ann[\"bbox\"][2] + ann[\"bbox\"][0]\n",
    "            ymax = ann[\"bbox\"][3] + ann[\"bbox\"][1]\n",
    "            \n",
    "            x = (xmin + xmax)/2\n",
    "            y = (ymin + ymax)/2\n",
    "            \n",
    "            w = xmax - xmin\n",
    "            h = ymax-ymin\n",
    "            \n",
    "            x = x * dw\n",
    "            w = w * dw\n",
    "            y = y * dh\n",
    "            h = h * dh\n",
    "            \n",
    "            mystring = str(str(ann['category_id']) + \" \" + str(truncate(x, 7)) + \" \" + str(truncate(y, 7)) + \" \" + str(truncate(w, 7)) + \" \" + str(truncate(h, 7)))\n",
    "            myfile.write(mystring)\n",
    "            myfile.write(\"\\n\")\n",
    "\n",
    "        myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!WARNING: hardcoded fix for islands dataset\n",
      "!WARNING: hardcoded fix for islands dataset\n",
      "!WARNING: hardcoded fix for islands dataset\n"
     ]
    }
   ],
   "source": [
    "createLabelsSingle(train, train_path, 'train/', d)\n",
    "createLabelsSingle(validate, val_path, 'val/', d)\n",
    "createLabelsSingle(test, test_path, 'test/', d)\n",
    "\n",
    "# sanity check: \n",
    "assert((len(train)+ len(validate) + len(test)) == (len(glob.glob(label_path + \"/test/*\")) + len(glob.glob(label_path + \"/train/*\")) + len(glob.glob(label_path + \"/val/*\"))))\n",
    "\n",
    "# TODO perpaps add labels?\n",
    "with open(Path(label_path).parent.__str__() + '/description', 'w') as f:\n",
    "    f.write('train, val, test,\\n{}, {}, {}'.format(len(train), len(validate), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert((len(train)+ len(validate) + len(test)) == (len(glob.glob(label_path + \"/test/*\")) + len(glob.glob(label_path + \"/train/*\")) + len(glob.glob(label_path + \"/val/*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO auto generate YAML\n",
    "# import yaml\n",
    "\n",
    "# data = dict('path' : './data/islands',  # dataset root dir\n",
    "#     'train' : 'images/train2017',  # train images (relative to 'path') 128 images\n",
    "#     'val' : 'images/train2017',  # val images (relative to 'path') 128 images\n",
    "#     A = 'a',\n",
    "#     B = dict(\n",
    "#         C = 'c',\n",
    "#         D = 'd',\n",
    "#         E = 'e',\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# with open('data.yml', 'w') as outfile:\n",
    "#     yaml.dump(data, outfile, default_flow_style=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d488aad3bd47f31cc49211d239eea484ef57a3647b66c4c13e8e3612e9e7defd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('wildsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
