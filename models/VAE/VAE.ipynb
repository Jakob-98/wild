{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AntixK/PyTorch-VAE\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import glob \n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import math\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from abc import abstractmethod\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional, Sequence, Union, Any, Callable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.utils as vutils\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params = {\n",
    "  'LR': 0.005,\n",
    "  'weight_decay': 0.0,\n",
    "  'scheduler_gamma': 0.95,\n",
    "  'kld_weight': 0.00025,\n",
    "  'manual_seed': 1265\n",
    "}\n",
    "\n",
    "\n",
    "class trainer_params:\n",
    "  gpus: 1\n",
    "  max_epochs: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENA(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform):\n",
    "        if not isinstance(root, tuple): raise # FIX THIS WHY TUPLE....\n",
    "        root = root[0]\n",
    "        self.root = root\n",
    "        self.transforms = transform\n",
    "        self.ids = [os.path.split(i)[1].split('.jpg')[0] for i in glob.glob(root + '/*.jpg', recursive=True)]\n",
    "        self.labelpath = Path(root).parent.parent / \"labels\" / Path(root).name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        imgname = img_id + '.jpg'\n",
    "\n",
    "        with open(self.labelpath / (img_id + '.txt')) as f:\n",
    "            target = f.readline()[0]\n",
    "        assert (len(target)==1)\n",
    "        target=int(target)\n",
    "        #TODO Check if convert RGB makes sense for Grayscale\n",
    "        img = Image.open(os.path.join(self.root, imgname)).convert('RGB')\n",
    "        img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning data module \n",
    "    Args:\n",
    "        data_dir: root directory of your dataset.\n",
    "        train_batch_size: the batch size to use during training.\n",
    "        val_batch_size: the batch size to use during validation.\n",
    "        patch_size: the size of the crop to take from the original images.\n",
    "        num_workers: the number of parallel workers to create to load data\n",
    "            items (see PyTorch's Dataloader documentation for more details).\n",
    "        pin_memory: whether prepared items should be loaded into pinned memory\n",
    "            or not. This can improve performance on GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dir: str,\n",
    "        val_dir: str,\n",
    "        test_dir: str,\n",
    "        train_batch_size: int = 8,\n",
    "        val_batch_size: int = 8,\n",
    "        # patch_size: Union[int, Sequence[int]] = (256, 256),\n",
    "        patch_size: int = 256, # Union[int, Sequence[int]] = (256, 256),\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print('!WARNING: hardcoded size')\n",
    "        self.train_dir = train_dir,\n",
    "        self.val_dir = val_dir,\n",
    "        self.test_dir = test_dir,        \n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "    \n",
    "        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.CenterCrop(148),\n",
    "                                              transforms.Resize(self.patch_size),\n",
    "                                              transforms.ToTensor(),])\n",
    "        \n",
    "        val_transforms = transforms.Compose([transforms.CenterCrop(148),\n",
    "                                            transforms.Resize(self.patch_size),\n",
    "                                            transforms.ToTensor(),])\n",
    "        \n",
    "        self.train_dataset = ENA(\n",
    "            self.train_dir,\n",
    "            # split='train',\n",
    "            transform=train_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "        \n",
    "        # Replace CelebA with your dataset\n",
    "        self.val_dataset = ENA(\n",
    "            self.val_dir,\n",
    "            transform=val_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "\n",
    "        self.test_dataset = ENA(\n",
    "            self.test_dir,\n",
    "            transform=val_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=144,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 im_size = 64,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        lin_lookup = {64: 4, 128: 16, 256: 64, 512: 256}\n",
    "        view_lookup = {64: 2, 128: 4, 256: 8, 512: 16}\n",
    "        if im_size not in lin_lookup: raise \n",
    "        self.lin_corr = lin_lookup.get(im_size)\n",
    "        self.view_corr = view_lookup.get(im_size)\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*self.lin_corr, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*self.lin_corr, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * self.lin_corr)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, self.view_corr, self.view_corr)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEXperiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vae_model: BaseVAE,\n",
    "                 params: dict) -> None:\n",
    "        super(VAEXperiment, self).__init__()\n",
    "\n",
    "        self.model = vae_model\n",
    "        self.params = params\n",
    "        self.curr_device = None\n",
    "        self.hold_graph = False\n",
    "        try:\n",
    "            self.hold_graph = self.params['retain_first_backpass']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        return self.model(input, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        train_loss = self.model.loss_function(*results,\n",
    "                                              M_N = self.params['kld_weight'], #al_img.shape[0]/ self.num_train_imgs,\n",
    "                                              optimizer_idx=optimizer_idx,\n",
    "                                              batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({key: val.item() for key, val in train_loss.items()}, sync_dist=True)\n",
    "\n",
    "        return train_loss['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        val_loss = self.model.loss_function(*results,\n",
    "                                            M_N = 1.0, #real_img.shape[0]/ self.num_val_imgs,\n",
    "                                            optimizer_idx = optimizer_idx,\n",
    "                                            batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({f\"val_{key}\": val.item() for key, val in val_loss.items()}, sync_dist=True)\n",
    "\n",
    "        \n",
    "    def on_validation_end(self) -> None:\n",
    "        self.sample_images()\n",
    "        \n",
    "    def sample_images(self):\n",
    "        # Get sample reconstruction image            \n",
    "        test_input, test_label = next(iter(self.trainer.datamodule.test_dataloader()))\n",
    "        test_input = test_input.to(self.curr_device)\n",
    "        test_label = test_label.to(self.curr_device)\n",
    "\n",
    "#         test_input, test_label = batch\n",
    "        recons = self.model.generate(test_input, labels = test_label)\n",
    "        Path(f\"{self.logger.log_dir}/Reconstructions\").mkdir(exist_ok=True, parents=True)\n",
    "        vutils.save_image(recons.data,\n",
    "                          os.path.join(self.logger.log_dir , \n",
    "                                       \"Reconstructions\", \n",
    "                                       f\"recons_{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                          normalize=True,\n",
    "                          nrow=12)\n",
    "\n",
    "        try:\n",
    "            samples = self.model.sample(144,\n",
    "                                        self.curr_device,\n",
    "                                        labels = test_label)\n",
    "            Path(f\"{self.logger.log_dir}/Samples\").mkdir(exist_ok=True, parents=True)\n",
    "            vutils.save_image(samples.cpu().data,\n",
    "                              os.path.join(self.logger.log_dir , \n",
    "                                           \"Samples\",      \n",
    "                                           f\"{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                              normalize=True,\n",
    "                              nrow=12)\n",
    "        except Warning:\n",
    "            pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optims = []\n",
    "        scheds = []\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.params['LR'],\n",
    "                               weight_decay=self.params['weight_decay'])\n",
    "        optims.append(optimizer)\n",
    "        # Check if more than 1 optimizer is required (Used for adversarial training)\n",
    "        try:\n",
    "            if self.params['LR_2'] is not None:\n",
    "                optimizer2 = optim.Adam(getattr(self.model,self.params['submodel']).parameters(),\n",
    "                                        lr=self.params['LR_2'])\n",
    "                optims.append(optimizer2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if self.params['scheduler_gamma'] is not None:\n",
    "                scheduler = optim.lr_scheduler.ExponentialLR(optims[0],\n",
    "                                                             gamma = self.params['scheduler_gamma'])\n",
    "                scheds.append(scheduler)\n",
    "\n",
    "                # Check if another scheduler is required for the second optimizer\n",
    "                try:\n",
    "                    if self.params['scheduler_gamma_2'] is not None:\n",
    "                        scheduler2 = optim.lr_scheduler.ExponentialLR(optims[1],\n",
    "                                                                      gamma = self.params['scheduler_gamma_2'])\n",
    "                        scheds.append(scheduler2)\n",
    "                except:\n",
    "                    pass\n",
    "                return optims, scheds\n",
    "        except:\n",
    "            return optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './'\n",
    "tb_logger =  TensorBoardLogger(save_dir=LOG_DIR,\n",
    "                               name='lightning_logs',)\n",
    "\n",
    "Path(f\"{tb_logger.log_dir}/Samples\").mkdir(exist_ok=True, parents=True)\n",
    "Path(f\"{tb_logger.log_dir}/Reconstructions\").mkdir(exist_ok=True, parents=True)\n",
    "model = VanillaVAE(3,128, im_size=256)\n",
    "experiment = VAEXperiment(model,\n",
    "                          exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "runner = Trainer(callbacks=[\n",
    "                     LearningRateMonitor(),\n",
    "                     ModelCheckpoint(save_top_k=2, \n",
    "                                     dirpath =os.path.join(LOG_DIR , \"checkpoints\"), \n",
    "                                     monitor= \"val_loss\",\n",
    "                                     save_last= True),\n",
    "                 ],\n",
    "                 gpus = 1, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!WARNING: hardcoded size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:611: UserWarning: Checkpoint directory C:\\Projects\\wild\\models\\VAE\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | VanillaVAE | 15.8 M\n",
      "-------------------------------------\n",
      "15.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.059    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace8b5597c024d158e879087c98d78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\temp\\\\ena\\\\labels\\\\256test\\\\1003.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\wild\\models\\VAE\\VAE.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000010?line=0'>1</a>\u001b[0m datamodule\u001b[39m=\u001b[39mVAEDataset(train_dir\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/temp/ena/images/256train100/\u001b[39m\u001b[39m'\u001b[39m, val_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/temp/ena/images/256test/\u001b[39m\u001b[39m'\u001b[39m, test_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/temp/ena/images/256test/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000010?line=1'>2</a>\u001b[0m datamodule\u001b[39m.\u001b[39msetup()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000010?line=2'>3</a>\u001b[0m runner\u001b[39m.\u001b[39;49mfit(experiment, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=812'>813</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1237'>1238</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1341'>1342</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1343'>1344</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1346'>1347</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1347'>1348</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1413\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1410'>1411</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1411'>1412</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m-> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1412'>1413</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1414'>1415</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1416'>1417</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:112\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=109'>110</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=110'>111</a>\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=111'>112</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=113'>114</a>\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=183'>184</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:259\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=255'>256</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=256'>257</a>\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=257'>258</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=258'>259</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=259'>260</a>\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=260'>261</a>\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:273\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=270'>271</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=271'>272</a>\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=272'>273</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=273'>274</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=274'>275</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/pytorch_lightning/utilities/fetching.py?line=275'>276</a>\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Projects/pyvenvs/wildsenv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Projects\\wild\\models\\VAE\\VAE.ipynb Cell 3'\u001b[0m in \u001b[0;36mENA.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000002?line=11'>12</a>\u001b[0m img_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids[index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000002?line=13'>14</a>\u001b[0m imgname \u001b[39m=\u001b[39m img_id \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000002?line=15'>16</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabelpath \u001b[39m/\u001b[39;49m (img_id \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000002?line=16'>17</a>\u001b[0m     target \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadline()[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/wild/models/VAE/VAE.ipynb#ch0000002?line=17'>18</a>\u001b[0m \u001b[39massert\u001b[39;00m (\u001b[39mlen\u001b[39m(target)\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\temp\\\\ena\\\\labels\\\\256test\\\\1003.txt'"
     ]
    }
   ],
   "source": [
    "datamodule=VAEDataset(train_dir= 'C:/temp/ena/images/256train100/', val_dir='C:/temp/ena/images/256test/', test_dir='C:/temp/ena/images/256test/')\n",
    "datamodule.setup()\n",
    "runner.fit(experiment, datamodule=datamodule)\n",
    "#TODO labels for val ofc dont match test...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d488aad3bd47f31cc49211d239eea484ef57a3647b66c4c13e8e3612e9e7defd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('wildsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
