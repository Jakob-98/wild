{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AntixK/PyTorch-VAE\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import glob \n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import math\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from abc import abstractmethod\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional, Sequence, Union, Any, Callable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.utils as vutils\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params = {\n",
    "  'LR': 0.005,\n",
    "  'weight_decay': 0.0,\n",
    "  'scheduler_gamma': 0.95,\n",
    "  'kld_weight': 0.00025,\n",
    "  'manual_seed': 1265\n",
    "}\n",
    "\n",
    "\n",
    "class trainer_params:\n",
    "  gpus: 1\n",
    "  max_epochs: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENA(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform):\n",
    "        if not isinstance(root, tuple): raise # FIX THIS WHY TUPLE....\n",
    "        root = root[0]\n",
    "        self.root = root\n",
    "        self.transforms = transform\n",
    "        self.ids = [os.path.split(i)[1].split('.jpg')[0] for i in glob.glob(root + '/*.jpg', recursive=True)]\n",
    "        self.labelpath = Path(root).parent.parent / \"labels\" / Path(root).name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        imgname = img_id + '.jpg'\n",
    "\n",
    "        with open(self.labelpath / (img_id + '.txt')) as f:\n",
    "            target = f.readline()[0]\n",
    "        assert (len(target)==1)\n",
    "        target=int(target)\n",
    "        #TODO Check if convert RGB makes sense for Grayscale\n",
    "        img = Image.open(os.path.join(self.root, imgname)).convert('RGB')\n",
    "        img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning data module \n",
    "    Args:\n",
    "        data_dir: root directory of your dataset.\n",
    "        train_batch_size: the batch size to use during training.\n",
    "        val_batch_size: the batch size to use during validation.\n",
    "        patch_size: the size of the crop to take from the original images.\n",
    "        num_workers: the number of parallel workers to create to load data\n",
    "            items (see PyTorch's Dataloader documentation for more details).\n",
    "        pin_memory: whether prepared items should be loaded into pinned memory\n",
    "            or not. This can improve performance on GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dir: str,\n",
    "        val_dir: str,\n",
    "        test_dir: str,\n",
    "        train_batch_size: int = 8,\n",
    "        val_batch_size: int = 8,\n",
    "        # patch_size: Union[int, Sequence[int]] = (256, 256),\n",
    "        patch_size: int = 64, # Union[int, Sequence[int]] = (256, 256),\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print('!WARNING: hardcoded size')\n",
    "        self.train_dir = train_dir,\n",
    "        self.val_dir = val_dir,\n",
    "        self.test_dir = test_dir,        \n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "    \n",
    "        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.CenterCrop(148),\n",
    "                                              transforms.Resize(self.patch_size),\n",
    "                                              transforms.ToTensor(),])\n",
    "        \n",
    "        val_transforms = transforms.Compose([transforms.CenterCrop(148),\n",
    "                                            transforms.Resize(self.patch_size),\n",
    "                                            transforms.ToTensor(),])\n",
    "        \n",
    "        self.train_dataset = ENA(\n",
    "            self.train_dir,\n",
    "            # split='train',\n",
    "            transform=train_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "        \n",
    "        # Replace CelebA with your dataset\n",
    "        self.val_dataset = ENA(\n",
    "            self.val_dir,\n",
    "            transform=val_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "\n",
    "        self.test_dataset = ENA(\n",
    "            self.test_dir,\n",
    "            transform=val_transforms,\n",
    "            # download=False,\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=144,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEXperiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vae_model: BaseVAE,\n",
    "                 params: dict) -> None:\n",
    "        super(VAEXperiment, self).__init__()\n",
    "\n",
    "        self.model = vae_model\n",
    "        self.params = params\n",
    "        self.curr_device = None\n",
    "        self.hold_graph = False\n",
    "        try:\n",
    "            self.hold_graph = self.params['retain_first_backpass']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        return self.model(input, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        train_loss = self.model.loss_function(*results,\n",
    "                                              M_N = self.params['kld_weight'], #al_img.shape[0]/ self.num_train_imgs,\n",
    "                                              optimizer_idx=optimizer_idx,\n",
    "                                              batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({key: val.item() for key, val in train_loss.items()}, sync_dist=True)\n",
    "\n",
    "        return train_loss['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        val_loss = self.model.loss_function(*results,\n",
    "                                            M_N = 1.0, #real_img.shape[0]/ self.num_val_imgs,\n",
    "                                            optimizer_idx = optimizer_idx,\n",
    "                                            batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({f\"val_{key}\": val.item() for key, val in val_loss.items()}, sync_dist=True)\n",
    "\n",
    "        \n",
    "    def on_validation_end(self) -> None:\n",
    "        self.sample_images()\n",
    "        \n",
    "    def sample_images(self):\n",
    "        # Get sample reconstruction image            \n",
    "        test_input, test_label = next(iter(self.trainer.datamodule.test_dataloader()))\n",
    "        test_input = test_input.to(self.curr_device)\n",
    "        test_label = test_label.to(self.curr_device)\n",
    "\n",
    "#         test_input, test_label = batch\n",
    "        recons = self.model.generate(test_input, labels = test_label)\n",
    "        Path(f\"{self.logger.log_dir}/Reconstructions\").mkdir(exist_ok=True, parents=True)\n",
    "        vutils.save_image(recons.data,\n",
    "                          os.path.join(self.logger.log_dir , \n",
    "                                       \"Reconstructions\", \n",
    "                                       f\"recons_{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                          normalize=True,\n",
    "                          nrow=12)\n",
    "\n",
    "        try:\n",
    "            samples = self.model.sample(144,\n",
    "                                        self.curr_device,\n",
    "                                        labels = test_label)\n",
    "            Path(f\"{self.logger.log_dir}/Samples\").mkdir(exist_ok=True, parents=True)\n",
    "            vutils.save_image(samples.cpu().data,\n",
    "                              os.path.join(self.logger.log_dir , \n",
    "                                           \"Samples\",      \n",
    "                                           f\"{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                              normalize=True,\n",
    "                              nrow=12)\n",
    "        except Warning:\n",
    "            pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optims = []\n",
    "        scheds = []\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.params['LR'],\n",
    "                               weight_decay=self.params['weight_decay'])\n",
    "        optims.append(optimizer)\n",
    "        # Check if more than 1 optimizer is required (Used for adversarial training)\n",
    "        try:\n",
    "            if self.params['LR_2'] is not None:\n",
    "                optimizer2 = optim.Adam(getattr(self.model,self.params['submodel']).parameters(),\n",
    "                                        lr=self.params['LR_2'])\n",
    "                optims.append(optimizer2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if self.params['scheduler_gamma'] is not None:\n",
    "                scheduler = optim.lr_scheduler.ExponentialLR(optims[0],\n",
    "                                                             gamma = self.params['scheduler_gamma'])\n",
    "                scheds.append(scheduler)\n",
    "\n",
    "                # Check if another scheduler is required for the second optimizer\n",
    "                try:\n",
    "                    if self.params['scheduler_gamma_2'] is not None:\n",
    "                        scheduler2 = optim.lr_scheduler.ExponentialLR(optims[1],\n",
    "                                                                      gamma = self.params['scheduler_gamma_2'])\n",
    "                        scheds.append(scheduler2)\n",
    "                except:\n",
    "                    pass\n",
    "                return optims, scheds\n",
    "        except:\n",
    "            return optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './'\n",
    "tb_logger =  TensorBoardLogger(save_dir=LOG_DIR,\n",
    "                               name='lightning_logs',)\n",
    "\n",
    "Path(f\"{tb_logger.log_dir}/Samples\").mkdir(exist_ok=True, parents=True)\n",
    "Path(f\"{tb_logger.log_dir}/Reconstructions\").mkdir(exist_ok=True, parents=True)\n",
    "model = VanillaVAE(3,128)\n",
    "experiment = VAEXperiment(model,\n",
    "                          exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "runner = Trainer(callbacks=[\n",
    "                     LearningRateMonitor(),\n",
    "                     ModelCheckpoint(save_top_k=2, \n",
    "                                     dirpath =os.path.join(LOG_DIR , \"checkpoints\"), \n",
    "                                     monitor= \"val_loss\",\n",
    "                                     save_last= True),\n",
    "                 ],\n",
    "                 gpus = 1, max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | VanillaVAE | 3.9 M \n",
      "-------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.751    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!WARNING: hardcoded size\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a4d17227344928a71e7ceac23336d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Projects\\pyvenvs\\wildsenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (38) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bccfa5a9fcd4e4c9417d566ed49ee07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44020fcaccf1456c8379611ea1202b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dab217671b744b8b7124f966f2b657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datamodule=VAEDataset(train_dir= 'C:/temp/ena/images/train5/', val_dir='C:/temp/ena/images/val/', test_dir='C:/temp/ena/images/test/')\n",
    "datamodule.setup()\n",
    "runner.fit(experiment, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = next(iter(datamodule.train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d488aad3bd47f31cc49211d239eea484ef57a3647b66c4c13e8e3612e9e7defd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('wildsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
