{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AntixK/PyTorch-VAE\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import glob \n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import math\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from abc import abstractmethod\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.utils as vutils\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp_params:\n",
    "  LR: 0.005\n",
    "  weight_decay: 0.0\n",
    "  scheduler_gamma: 0.95\n",
    "  kld_weight: 0.00025\n",
    "  manual_seed: 1265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaVAE(3, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "runner = Trainer(callbacks=[\n",
    "                     LearningRateMonitor(),\n",
    "                     ModelCheckpoint(save_top_k=2, \n",
    "                                     dirpath =os.path.join('./' , \"checkpoints\"), \n",
    "                                     monitor= \"val_loss\",\n",
    "                                     save_last= True),\n",
    "                 ],\n",
    "                #  strategy=DDPPlugin(find_unused_parameters=False),\n",
    "                 gpus = 1, max_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEXperiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vae_model: BaseVAE,\n",
    "                 params: dict) -> None:\n",
    "        super(VAEXperiment, self).__init__()\n",
    "\n",
    "        self.model = vae_model\n",
    "        self.params = params\n",
    "        self.curr_device = None\n",
    "        self.hold_graph = False\n",
    "        try:\n",
    "            self.hold_graph = self.params['retain_first_backpass']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        return self.model(input, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        train_loss = self.model.loss_function(*results,\n",
    "                                              M_N = self.params['kld_weight'], #al_img.shape[0]/ self.num_train_imgs,\n",
    "                                              optimizer_idx=optimizer_idx,\n",
    "                                              batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({key: val.item() for key, val in train_loss.items()}, sync_dist=True)\n",
    "\n",
    "        return train_loss['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        val_loss = self.model.loss_function(*results,\n",
    "                                            M_N = 1.0, #real_img.shape[0]/ self.num_val_imgs,\n",
    "                                            optimizer_idx = optimizer_idx,\n",
    "                                            batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({f\"val_{key}\": val.item() for key, val in val_loss.items()}, sync_dist=True)\n",
    "\n",
    "        \n",
    "    def on_validation_end(self) -> None:\n",
    "        self.sample_images()\n",
    "        \n",
    "    def sample_images(self):\n",
    "        # Get sample reconstruction image            \n",
    "        test_input, test_label = next(iter(self.trainer.datamodule.test_dataloader()))\n",
    "        test_input = test_input.to(self.curr_device)\n",
    "        test_label = test_label.to(self.curr_device)\n",
    "\n",
    "#         test_input, test_label = batch\n",
    "        recons = self.model.generate(test_input, labels = test_label)\n",
    "        vutils.save_image(recons.data,\n",
    "                          os.path.join(self.logger.log_dir , \n",
    "                                       \"Reconstructions\", \n",
    "                                       f\"recons_{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                          normalize=True,\n",
    "                          nrow=12)\n",
    "\n",
    "        try:\n",
    "            samples = self.model.sample(144,\n",
    "                                        self.curr_device,\n",
    "                                        labels = test_label)\n",
    "            vutils.save_image(samples.cpu().data,\n",
    "                              os.path.join(self.logger.log_dir , \n",
    "                                           \"Samples\",      \n",
    "                                           f\"{self.logger.name}_Epoch_{self.current_epoch}.png\"),\n",
    "                              normalize=True,\n",
    "                              nrow=12)\n",
    "        except Warning:\n",
    "            pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optims = []\n",
    "        scheds = []\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.params['LR'],\n",
    "                               weight_decay=self.params['weight_decay'])\n",
    "        optims.append(optimizer)\n",
    "        # Check if more than 1 optimizer is required (Used for adversarial training)\n",
    "        try:\n",
    "            if self.params['LR_2'] is not None:\n",
    "                optimizer2 = optim.Adam(getattr(self.model,self.params['submodel']).parameters(),\n",
    "                                        lr=self.params['LR_2'])\n",
    "                optims.append(optimizer2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if self.params['scheduler_gamma'] is not None:\n",
    "                scheduler = optim.lr_scheduler.ExponentialLR(optims[0],\n",
    "                                                             gamma = self.params['scheduler_gamma'])\n",
    "                scheds.append(scheduler)\n",
    "\n",
    "                # Check if another scheduler is required for the second optimizer\n",
    "                try:\n",
    "                    if self.params['scheduler_gamma_2'] is not None:\n",
    "                        scheduler2 = optim.lr_scheduler.ExponentialLR(optims[1],\n",
    "                                                                      gamma = self.params['scheduler_gamma_2'])\n",
    "                        scheds.append(scheduler2)\n",
    "                except:\n",
    "                    pass\n",
    "                return optims, scheds\n",
    "        except:\n",
    "            return optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = VAEXperiment(model,\n",
    "                          exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.fit(experiment, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENA(Dataset):\n",
    "\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.ids = [os.path.split(i)[1].split('.jpg')[0] for i in glob.glob(root + '/*.jpg', recursive=True)]\n",
    "        self.labelpath = Path(root).parent.parent / \"labels\" / Path(root).name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        imgname = img_id + '.jpg'\n",
    "\n",
    "        with open(self.labelpath / (img_id + '.txt')) as f:\n",
    "            target = f.readline()[0]\n",
    "\n",
    "        #TODO Check if convert RGB makes sense for Grayscale\n",
    "        img = Image.open(os.path.join(self.root, imgname)).convert('RGB')\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ENA(root = 'C:/temp/ena_full/', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENAData = ENA(root = 'C:/temp/ena/images/train5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=2048x1536>, '1')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENAData.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/temp/ena/images/train5', '')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1054',\n",
       " '1060',\n",
       " '107',\n",
       " '1076',\n",
       " '111',\n",
       " '1129',\n",
       " '1137',\n",
       " '116',\n",
       " '1175',\n",
       " '1208',\n",
       " '1247',\n",
       " '1294',\n",
       " '1310',\n",
       " '1332',\n",
       " '1374',\n",
       " '1379',\n",
       " '146',\n",
       " '1467',\n",
       " '1473',\n",
       " '1493',\n",
       " '1517',\n",
       " '1708',\n",
       " '1768',\n",
       " '178',\n",
       " '1819',\n",
       " '1841',\n",
       " '1897',\n",
       " '1901',\n",
       " '1909',\n",
       " '1914',\n",
       " '194',\n",
       " '1971',\n",
       " '1972',\n",
       " '2002',\n",
       " '2005',\n",
       " '2011',\n",
       " '2013',\n",
       " '2031',\n",
       " '2085',\n",
       " '2115',\n",
       " '2125',\n",
       " '2155',\n",
       " '2190',\n",
       " '2206',\n",
       " '2246',\n",
       " '2376',\n",
       " '2387',\n",
       " '2416',\n",
       " '2426',\n",
       " '2488',\n",
       " '2533',\n",
       " '2583',\n",
       " '2591',\n",
       " '2592',\n",
       " '2605',\n",
       " '2642',\n",
       " '2762',\n",
       " '2765',\n",
       " '2788',\n",
       " '279',\n",
       " '2799',\n",
       " '2840',\n",
       " '285',\n",
       " '2979',\n",
       " '299',\n",
       " '2992',\n",
       " '3017',\n",
       " '3018',\n",
       " '3069',\n",
       " '3074',\n",
       " '3080',\n",
       " '3086',\n",
       " '3192',\n",
       " '3210',\n",
       " '3258',\n",
       " '3275',\n",
       " '3300',\n",
       " '3323',\n",
       " '3332',\n",
       " '3363',\n",
       " '3406',\n",
       " '3437',\n",
       " '3462',\n",
       " '3519',\n",
       " '3550',\n",
       " '3642',\n",
       " '3644',\n",
       " '3657',\n",
       " '3661',\n",
       " '3720',\n",
       " '3728',\n",
       " '3741',\n",
       " '3745',\n",
       " '377',\n",
       " '3779',\n",
       " '379',\n",
       " '3799',\n",
       " '3819',\n",
       " '3897',\n",
       " '3940',\n",
       " '3975',\n",
       " '408',\n",
       " '4357',\n",
       " '437',\n",
       " '4401',\n",
       " '4405',\n",
       " '4448',\n",
       " '4450',\n",
       " '4459',\n",
       " '447',\n",
       " '4502',\n",
       " '4555',\n",
       " '4566',\n",
       " '4614',\n",
       " '4618',\n",
       " '4630',\n",
       " '4666',\n",
       " '4678',\n",
       " '4719',\n",
       " '49',\n",
       " '4976',\n",
       " '4980',\n",
       " '5109',\n",
       " '5116',\n",
       " '5128',\n",
       " '5187',\n",
       " '5204',\n",
       " '5217',\n",
       " '5241',\n",
       " '5244',\n",
       " '5267',\n",
       " '5284',\n",
       " '5335',\n",
       " '535',\n",
       " '5363',\n",
       " '537',\n",
       " '5373',\n",
       " '538',\n",
       " '5540',\n",
       " '5564',\n",
       " '557',\n",
       " '5576',\n",
       " '5631',\n",
       " '5652',\n",
       " '5689',\n",
       " '5709',\n",
       " '5714',\n",
       " '5730',\n",
       " '5762',\n",
       " '5821',\n",
       " '588',\n",
       " '5889',\n",
       " '5911',\n",
       " '593',\n",
       " '5938',\n",
       " '5942',\n",
       " '5949',\n",
       " '5959',\n",
       " '5977',\n",
       " '6026',\n",
       " '6046',\n",
       " '6051',\n",
       " '6054',\n",
       " '6099',\n",
       " '6113',\n",
       " '6157',\n",
       " '6214',\n",
       " '6278',\n",
       " '6281',\n",
       " '6294',\n",
       " '6301',\n",
       " '6331',\n",
       " '6352',\n",
       " '6373',\n",
       " '6383',\n",
       " '6416',\n",
       " '6420',\n",
       " '6463',\n",
       " '6493',\n",
       " '6508',\n",
       " '6558',\n",
       " '664',\n",
       " '6646',\n",
       " '6662',\n",
       " '6687',\n",
       " '6720',\n",
       " '6764',\n",
       " '6771',\n",
       " '6822',\n",
       " '6845',\n",
       " '6871',\n",
       " '6882',\n",
       " '6883',\n",
       " '6897',\n",
       " '6945',\n",
       " '6973',\n",
       " '7042',\n",
       " '7053',\n",
       " '7107',\n",
       " '7135',\n",
       " '7164',\n",
       " '7175',\n",
       " '7238',\n",
       " '7243',\n",
       " '7245',\n",
       " '7264',\n",
       " '7274',\n",
       " '729',\n",
       " '7303',\n",
       " '7308',\n",
       " '7338',\n",
       " '7428',\n",
       " '7449',\n",
       " '7451',\n",
       " '7452',\n",
       " '7512',\n",
       " '7516',\n",
       " '7527',\n",
       " '7546',\n",
       " '7592',\n",
       " '7649',\n",
       " '7719',\n",
       " '7721',\n",
       " '7733',\n",
       " '775',\n",
       " '777',\n",
       " '7781',\n",
       " '7810',\n",
       " '7813',\n",
       " '7843',\n",
       " '7846',\n",
       " '7866',\n",
       " '7871',\n",
       " '7916',\n",
       " '7970',\n",
       " '7984',\n",
       " '8000',\n",
       " '8006',\n",
       " '8009',\n",
       " '8075',\n",
       " '810',\n",
       " '8107',\n",
       " '8117',\n",
       " '815',\n",
       " '8163',\n",
       " '8241',\n",
       " '8270',\n",
       " '8359',\n",
       " '8362',\n",
       " '8373',\n",
       " '8431',\n",
       " '8450',\n",
       " '8469',\n",
       " '8479',\n",
       " '8498',\n",
       " '8522',\n",
       " '8526',\n",
       " '8554',\n",
       " '856',\n",
       " '8597',\n",
       " '8661',\n",
       " '8724',\n",
       " '8737',\n",
       " '8750',\n",
       " '8891',\n",
       " '8902',\n",
       " '8929',\n",
       " '8984',\n",
       " '8989',\n",
       " '8990',\n",
       " '9072',\n",
       " '9084',\n",
       " '9090',\n",
       " '910',\n",
       " '9100',\n",
       " '9122',\n",
       " '9124',\n",
       " '9138',\n",
       " '9146',\n",
       " '9165',\n",
       " '9184',\n",
       " '9224',\n",
       " '9226',\n",
       " '9250',\n",
       " '9276',\n",
       " '928',\n",
       " '9315',\n",
       " '9326',\n",
       " '9365',\n",
       " '9427',\n",
       " '9428',\n",
       " '9435',\n",
       " '9453',\n",
       " '9469',\n",
       " '950',\n",
       " '9509',\n",
       " '954',\n",
       " '9604',\n",
       " '9649',\n",
       " '975']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.path.split(i)[1].split('.jpg')[0] for i in glob.glob(root + '/*.jpg', recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d488aad3bd47f31cc49211d239eea484ef57a3647b66c4c13e8e3612e9e7defd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('wildsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
